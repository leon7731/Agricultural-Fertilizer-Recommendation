{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Source: Kaggle\n",
    "# Link: https://www.kaggle.com/datasets/sanchitagholap/crop-and-fertilizer-dataset-for-westernmaharashtra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/Crop and fertilizer dataset.csv')\n",
    "\n",
    "# Removing leading and trailing spaces from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Replace multiple spaces with a single space in all column names\n",
    "df.columns = df.columns.str.replace(r'\\s+', ' ', regex=True)  \n",
    "\n",
    "# Drop Unnecessary Columns\n",
    "df.drop(['District_Name', 'Link'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Overview of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: EDA - Missing Values Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3)i): EDA - Show Missing Values in each Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get percentage of null values in each column\n",
    "null_values_percentage = df.isnull().mean().round(4).mul(100).sort_values(ascending=False)\n",
    "print('-' * 44)\n",
    "print(\"Percentage(%) of null values in each column\")\n",
    "print('-' * 44)\n",
    "print(null_values_percentage)\n",
    "print('\\n')\n",
    "\n",
    "# Get total null values in each column\n",
    "total_null_values = df.isnull().sum().sort_values(ascending=False)\n",
    "print('-' * 33)\n",
    "print(\"Total null values in each column\")\n",
    "print('-' * 33)\n",
    "print(total_null_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: EDA - Duplicate Values Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4)i): EDA - Show Duplicate Values Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get percentage of duplicate rows\n",
    "total_rows = len(df)\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "duplicate_percentage = (duplicate_rows / total_rows) * 100\n",
    "\n",
    "print('-' * 48)\n",
    "print(\"Percentage(%) of duplicate rows in the DataFrame\")\n",
    "print('-' * 48)\n",
    "print(f\"{duplicate_percentage:.2f}%\")\n",
    "print('\\n')\n",
    "\n",
    "# Get total number of duplicate rows\n",
    "print('-' * 30)\n",
    "print(\"Total number of duplicate rows\")\n",
    "print('-' * 30)\n",
    "print(duplicate_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: EDA - Analyzing Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5)i): EDA - Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_analysis_plotly(df):\n",
    "    \"\"\"\n",
    "\n",
    "    Perform univariate analysis on a DataFrame using Plotly.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "    - Interactive Plotly plots with summary statistics in the legend.\n",
    "    \"\"\"\n",
    "    colors = px.colors.qualitative.Plotly\n",
    "\n",
    "    for idx, column in enumerate(df.columns):\n",
    "\n",
    "        # Generate descriptive statistics\n",
    "        stats = df[column].describe()\n",
    "        stats_str = '<br>'.join([f'{k}: {v:.2f}' if isinstance(v, (float, int)) else f'{k}: {v}' for k, v in stats.items()])\n",
    "\n",
    "        # Visualization based on datatype\n",
    "        if np.issubdtype(df[column].dtype, np.number):\n",
    "\n",
    "            # If the column is numeric, plot a histogram with a box plot as marginal\n",
    "            fig = px.histogram(df, x=column, marginal=\"box\", title=f\"Histogram for {column}\", color_discrete_sequence=[colors[idx % len(colors)]])\n",
    "        else:\n",
    "\n",
    "            # If the column is categorical or textual, plot a bar chart\n",
    "            value_counts = df[column].value_counts()\n",
    "\n",
    "            fig = px.bar(x=value_counts.index, y=value_counts.values, \n",
    "                         title=f\"Bar Chart for {column}\", \n",
    "                         labels={\"x\": column, \"y\": \"Count\"},\n",
    "                         color_discrete_sequence=[colors[idx % len(colors)]])\n",
    "\n",
    "        # Add descriptive stats as a legend using a dummy trace for both types of columns\n",
    "        fig.add_trace(go.Scatter(x=[None], y=[None], mode=\"lines\", \n",
    "                                 name=stats_str, showlegend=True, \n",
    "                                 hoverinfo=\"none\", opacity=0))\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "univariate_analysis_plotly(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6): EDA - Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6)i): EDA - Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_auto_ordinal_encoding(df: pd.DataFrame, columns_to_encode: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply automatic Ordinal Encoding to specific columns of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input DataFrame\n",
    "    - columns_to_encode: List of column names to apply Ordinal Encoding\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with Ordinally Encoded columns\n",
    "    \"\"\"\n",
    "    \n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    for column in columns_to_encode:\n",
    "        unique_values = df[column].unique()\n",
    "        ordinal_mapping = {key: val for val, key in enumerate(unique_values)}\n",
    "        \n",
    "        # Print the ordinal mapping for the column\n",
    "        print(f\"Ordinal Encoding for '{column}': {ordinal_mapping}\")\n",
    "        \n",
    "        df_encoded[column] = df[column].map(ordinal_mapping)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "# Example usage:\n",
    "df_encoded_eda = apply_auto_ordinal_encoding(df, ['Soil_color', 'Crop', 'Fertilizer', ])\n",
    "df_encoded_eda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "def heatmap_correlations(df: pd.DataFrame, targetVariable:str ,colorscale:str=\"Viridis\"):\n",
    "    \"\"\"\n",
    "    Create a heatmap showing the correlation of all pairs of variables in the dataframe.\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Data to be plotted.\n",
    "    - targetVariable (str): The dependent variable for which correlations will be displayed.\n",
    "    - colorscale (str): Desired colorscale for the heatmap. Default is \"Viridis\".\n",
    "    Returns:\n",
    "    - None: Shows the heatmap.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filtering only numerical columns\n",
    "    df_numeric = df.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Check if targetVariable is numeric\n",
    "    if targetVariable not in df_numeric.columns:\n",
    "        print(f\"The target variable {targetVariable} is not numeric.\")\n",
    "        return\n",
    "    \n",
    "    # Print the correlation of the target variable with other variables\n",
    "    print('-' * 52)\n",
    "    print(f\"Correlation of {targetVariable} with other Independent variables\")\n",
    "    print('-' * 52)\n",
    "    print(df_numeric.corr()[targetVariable].sort_values(ascending=False))\n",
    "    \n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = df_numeric.corr()\n",
    "    # Create a heatmap using the correlation matrix\n",
    "    fig = ff.create_annotated_heatmap(\n",
    "        z=corr_matrix.values, \n",
    "        x=list(corr_matrix.columns), \n",
    "        y=list(corr_matrix.index),\n",
    "        annotation_text=corr_matrix.round(2).values,\n",
    "        colorscale=colorscale\n",
    "    )\n",
    "\n",
    "    fig.update_layout(title=\"Correlation Heatmap of Variables\")\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# heatmap_correlations(df, targetVariable=\"Class\", colorscale='RdYlGn')\n",
    "heatmap_correlations(df_encoded_eda, targetVariable=\"Fertilizer\", colorscale='RdYlGn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "----------------------------------------------------\n",
    "Correlation of Fertilizer with other Independent variables\n",
    "----------------------------------------------------\n",
    "Fertilizer     1.000000\n",
    "Crop           0.459263\n",
    "Soil_color     0.125628\n",
    "Potassium     -0.044753\n",
    "pH            -0.051754\n",
    "Rainfall      -0.103483\n",
    "Temperature   -0.124724\n",
    "Phosphorus    -0.174454\n",
    "Nitrogen      -0.214226\n",
    "Name: Fertilizer, dtype: float64\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# # Drop All Columns with Less 10% Positive and Negative Correlation\n",
    "# df.drop([\"Potassium\",\n",
    "#          \"pH\"\n",
    "#         ], axis=1, inplace=True)\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "\n",
    "# compute the vif for all given features\n",
    "def compute_vif(dataframe: pd.DataFrame, numerical_columns:list, sort_ascending:bool=True):\n",
    "    \"\"\"\n",
    "    Calculate the Variance Inflation Factor (VIF) for each feature in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe (pd.DataFrame): The DataFrame containing the features.\n",
    "    - numerical_columns (list): The list of numerical columns to calculate VIF for.\n",
    "    - sort_ascending (bool): Whether to sort the VIF scores in ascending order. Default is True.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = dataframe[numerical_columns]\n",
    "    # the calculation of variance inflation requires a constant\n",
    "    X['intercept'] = 1\n",
    "    \n",
    "    # create dataframe to store vif values\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"Variable\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    vif = vif[vif['Variable']!='intercept']\n",
    "    \n",
    "    # Sort the VIF data\n",
    "    vif.sort_values(by=\"VIF\", ascending=sort_ascending, inplace=True)\n",
    "    \n",
    "    return vif\n",
    "\n",
    "\n",
    "# Get Numerical Features\n",
    "numerical_features = df.select_dtypes(include=np.number).columns.tolist()  # Get all numeric columns\n",
    "\n",
    "\n",
    "# compute vif \n",
    "compute_vif(\n",
    "    dataframe=df,\n",
    "    numerical_columns=numerical_features,\n",
    "    sort_ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7): EDA - Feature Engineering/Scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7)i): EDA - Categorical Feature Engineering/Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "def apply_auto_ordinal_encoding(df: pd.DataFrame, columns_to_encode: list[str], save_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply automatic Ordinal Encoding to specific columns of a DataFrame and save the encoding mapping.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input DataFrame\n",
    "    - columns_to_encode: List of column names to apply Ordinal Encoding\n",
    "    - save_path: Path to save the encoding mapping\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with Ordinally Encoded columns\n",
    "    \"\"\"\n",
    "    \n",
    "    df_encoded = df.copy()\n",
    "    ordinal_mappings = {}\n",
    "    \n",
    "    for column in columns_to_encode:\n",
    "        unique_values = df[column].unique()\n",
    "        ordinal_mapping = {key: val for val, key in enumerate(unique_values)}\n",
    "        \n",
    "        # Print the ordinal mapping for the column\n",
    "        print(f\"Ordinal Encoding for '{column}': {ordinal_mapping}\")\n",
    "        \n",
    "        df_encoded[column] = df[column].map(ordinal_mapping)\n",
    "        ordinal_mappings[column] = ordinal_mapping\n",
    "    \n",
    "    # Save the ordinal mappings to a file\n",
    "    joblib.dump(ordinal_mappings, save_path)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "# Example usage:\n",
    "df_encoded = apply_auto_ordinal_encoding(df, \n",
    "                                         ['Soil_color', 'Crop', 'Fertilizer'], \n",
    "                                         'Mappings/ordinal_mappings.joblib')\n",
    "df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['Fertilizer'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7)ii): EDA - Numerical Feature Engineering/Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def min_max_scale_dataframe(dataframe, columns_to_scale):\n",
    "    \"\"\"\n",
    "    Scales the specified columns of the DataFrame using Min-Max Scaling.\n",
    "    :param dataframe: pandas DataFrame\n",
    "    :param columns_to_scale: list of strings, names of columns to scale\n",
    "    :return: DataFrame with scaled columns\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original one\n",
    "    df_scaled = dataframe.copy()\n",
    "    \n",
    "    # Initialize the Min-Max Scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Perform Min-Max Scaling on specified columns and update the DataFrame\n",
    "    df_scaled[columns_to_scale] = scaler.fit_transform(dataframe[columns_to_scale])\n",
    "    \n",
    "    return df_scaled\n",
    "\n",
    "\n",
    "numerical_features = [\n",
    "    \"Nitrogen\",\n",
    "    \"Phosphorus\",\n",
    "    \"Potassium\",\n",
    "    \"pH\",\n",
    "    \"Rainfall\",\n",
    "    \"Temperature\",\n",
    "]\n",
    "\n",
    "scaled_df = min_max_scale_dataframe(dataframe=df_encoded, \n",
    "                                    columns_to_scale=numerical_features)\n",
    "\n",
    "\n",
    "df = scaled_df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8) Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fertilizer'].value_counts().sort_index()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split - SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = df.drop(\"Fertilizer\", axis=1)\n",
    "y = df[\"Fertilizer\"]\n",
    "\n",
    "\n",
    "categorical_features_indices = [5]\n",
    "\n",
    "# Apply SMOTE-NC\n",
    "smote_nc = SMOTENC(sampling_strategy='auto', \n",
    "                   random_state=42, \n",
    "                   k_neighbors=5, \n",
    "                   n_jobs=-1,\n",
    "                   categorical_features=categorical_features_indices\n",
    "                   )\n",
    "\n",
    "X_resampled, y_resampled = smote_nc.fit_resample(X, y)\n",
    "\n",
    "# Split the resampled data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Generate distinct colors for each class using Plotly Express color scales\n",
    "def generate_colors(n):\n",
    "    color_scale = px.colors.qualitative.Plotly\n",
    "    return [color_scale[i % len(color_scale)] for i in range(n)]\n",
    "\n",
    "# Plot the class distribution before and after resampling\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Original Class Distribution\", \"Resampled Class Distribution after SMOTE\"))\n",
    "\n",
    "# Original class distribution\n",
    "original_counts = y.value_counts().sort_index()\n",
    "fig.add_trace(go.Bar(x=original_counts.index, y=original_counts.values, marker_color=generate_colors(len(original_counts)), showlegend=False), row=1, col=1)\n",
    "\n",
    "# Resampled class distribution\n",
    "resampled_counts = y_resampled.value_counts().sort_index()\n",
    "fig.add_trace(go.Bar(x=resampled_counts.index, y=resampled_counts.values, marker_color=generate_colors(len(resampled_counts)), showlegend=False), row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title_text=\"Class Distribution Before and After SMOTE\", xaxis_title=\"Exited\", yaxis_title=\"Count\")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9) XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def Confusion_Matrix_For_Multi_Class_With_Overview(title, y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Create a confusion matrix for multi-class classification with detailed overview.\n",
    "\n",
    "    Parameters:\n",
    "    - title: Title for the confusion matrix plot.\n",
    "    - y_test: True labels of the test data.\n",
    "    - y_pred: Predicted labels of the test data.\n",
    "\n",
    "    Returns:\n",
    "    - A seaborn heatmap representing the confusion matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # Creating the confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Determine class labels\n",
    "    class_labels = np.unique(np.concatenate((y_test, y_pred)))\n",
    "\n",
    "    # Calculate the counts and percentages for the confusion matrix\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n",
    "    \n",
    "    # Calculate TP and FP percentages\n",
    "    TP_percentages = [\"{0:.2%}\".format(value/np.sum(cf_matrix, axis=1)[i]) for i, value in enumerate(np.diag(cf_matrix))]\n",
    "    FP_percentages = [\"{0:.2%}\".format((np.sum(cf_matrix, axis=0)[i] - value)/np.sum(cf_matrix)) for i, value in enumerate(np.diag(cf_matrix))]\n",
    "    \n",
    "    # Combine TP and FP with their percentages\n",
    "    combined_info = []\n",
    "    for i in range(cf_matrix.shape[0]):\n",
    "        for j in range(cf_matrix.shape[1]):\n",
    "            value = cf_matrix[i, j]\n",
    "            if i == j:  # True Positive\n",
    "                combined_info.append(f\"{value}\\n(TP: {TP_percentages[i]})\")\n",
    "            else:  # False Positive\n",
    "                combined_info.append(f\"{value}\\n(FP: {FP_percentages[j]})\")\n",
    "\n",
    "    labels = np.asarray(combined_info).reshape(cf_matrix.shape)\n",
    "\n",
    "    # Plotting the heatmap\n",
    "    plt.figure(figsize=(25, 25))\n",
    "    ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    ax.set_title(f'{title}\\n\\n')\n",
    "    ax.set_xlabel('\\nPredicted Values')\n",
    "    ax.set_ylabel('Actual Values')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "def XGBoost_Train_Evaluate(X_train, X_test, y_train, y_test, \n",
    "                           objective='multi:softmax', num_class=None, \n",
    "                           n_estimators=100, learning_rate=0.1, max_depth=6, \n",
    "                           subsample=1.0, colsample_bytree=1.0, gamma=0, \n",
    "                           reg_alpha=0, reg_lambda=1, verbosity=1, random_state=42, verbose=False):\n",
    "   \n",
    "    model = xgb.XGBClassifier(\n",
    "        objective=objective,\n",
    "        num_class=num_class,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        gamma=gamma,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        verbosity=verbosity,\n",
    "        random_state=random_state,\n",
    "        use_label_encoder=False  # Avoids warning in newer versions of XGBoost\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='mlogloss', verbose=verbose)\n",
    "    y_pred = model.predict(X_test).flatten()  # Ensure y_pred is a 1D array\n",
    "    \n",
    "    df = pd.DataFrame({'Actual': y_test.tolist(), 'Predicted': y_pred.tolist()})\n",
    "    \n",
    "    Confusion_Matrix_For_Multi_Class_With_Overview(\"XGBoost Confusion Matrix\", y_test, y_pred)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "XGBoost_df = XGBoost_Train_Evaluate(X_train, X_test, y_train, y_test, \n",
    "                                    objective='multi:softmax', \n",
    "                                    num_class=19, \n",
    "                                    n_estimators=488, \n",
    "                                    learning_rate=0.013, \n",
    "                                    max_depth=6, \n",
    "                                    subsample=0.93, \n",
    "                                    colsample_bytree=0.50, \n",
    "                                    gamma=0.63, \n",
    "                                    reg_alpha=0.02, \n",
    "                                    reg_lambda=0.58, \n",
    "                                    verbosity=1, \n",
    "                                    random_state=42, \n",
    "                                    verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10) XGBoost Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# # Suppress Optuna output\n",
    "# optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "\n",
    "# # Objective function for Optuna\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         'objective': 'multi:softmax',\n",
    "#         'num_class': 19,  # Update based on your specific number of classes\n",
    "#         'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "#         'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.5),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "#         'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "#         'gamma': trial.suggest_float('gamma', 0, 10),\n",
    "#         'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "#         'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "#         'random_state': 42,\n",
    "#         'use_label_encoder': False,\n",
    "#         \"device\": \"gpu\",\n",
    "#         \"tree_method\": \"gpu_hist\"\n",
    "#     }\n",
    "#     model = xgb.XGBClassifier(**params)\n",
    "#     score = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "#     return score.mean()\n",
    "\n",
    "\n",
    "\n",
    "# # Create a study and optimize the objective function\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(objective, n_trials=500, timeout=None, show_progress_bar=True)\n",
    "\n",
    "# # Print the best parameters and the best score\n",
    "# print(\"Best Parameters:\", study.best_params)\n",
    "# print(\"Best Score:\", study.best_value)\n",
    "\n",
    "# # Get the detailed study results\n",
    "# df = study.trials_dataframe()\n",
    "# df_sorted = df.sort_values('value', ascending=False)\n",
    "# df_sorted.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
